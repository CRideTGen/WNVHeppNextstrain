Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cluster nodes: 20
Job stats:
job                 count    min threads    max threads
----------------  -------  -------------  -------------
all                     1              1              1
bowtie2                 1              1              1
ivar_consensus          1              1              1
samtools_index          1              1              1
samtools_sort           1              1              1
trim_reads_bbduk        1              1              1
total                   6              1              1

Select jobs to execute...
[Fri Sep  9 12:08:17 2022]

group job trim reads (jobs in lexicogr. order):

    [Fri Sep  9 12:08:17 2022]
    rule trim_reads_bbduk:
        input: read_symbolic_links/WNV.R1.fastq, read_symbolic_links/WNV.R2.fastq
        output: OutputDirectory/trimmed_reads/WNV_R1.fastq, OutputDirectory/trimmed_reads/WNV_R2.fastq
        jobid: 4
        wildcards: output_dir=OutputDirectory, sample_name=WNV
        resources: mem_mb=1000, disk_mb=1000, tmpdir=/tmp, memory=10000, time=00:5:00, cpus=1, ntasks=1

Submitted group job 16a79d8d-3678-5578-a852-c2455fc8c80a with external jobid 'Submitted batch job 60817'.
Terminating processes on user request, this might take some time.
No --cluster-cancel given. Will exit after finishing currently running jobs.
Cancelling snakemake on user request.
